{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from partition_decode.dataset import generate_gaussian_parity, recursive_gaussian_parity, generate_spirals, load_mnist\n",
    "from partition_decode.df_utils import get_tree_evals, get_forest_evals, get_forest_irm, get_tree_irm\n",
    "from partition_decode.models import ReluNetClassifier\n",
    "from partition_decode.metrics import irm2activations, score_matrix_representation, fast_evals\n",
    "import torch\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import zero_one_loss, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_to_params(h, k=2, d=2):\n",
    "    return (d+1) * h + (h+1) * k\n",
    "\n",
    "def get_units_at_interp(n, k=2, d=2):\n",
    "    # n k = (d+1) H + (H+1) K\n",
    "    return np.ceil((n * k - k) / (d + 2)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "n_train = 4000\n",
    "\n",
    "train_set = datasets.MNIST('/mnt/ssd3/ronan/pytorch', train=True, download=True)\n",
    "X_train = train_set.data.numpy()[:n_train]\n",
    "X_train = X_train.reshape((n_train, -1))\n",
    "y_train = train_set.targets.numpy()[:n_train]\n",
    "del train_set\n",
    "\n",
    "test_set = datasets.MNIST('/mnt/ssd3/ronan/pytorch', train=False, download=True)\n",
    "X_test = test_set.data.numpy()\n",
    "X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "y_test = test_set.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 hidden units\n"
     ]
    }
   ],
   "source": [
    "n_hidden = get_units_at_interp(n_train, k=len(np.unique(y_train)), d=X_train.shape[-1])\n",
    "print(f'{n_hidden} hidden units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eigenval_metrics(irm):\n",
    "    metric_params = [\n",
    "        {'metric': 'norm', 'p': 1},\n",
    "        {'metric': 'norm', 'p': 2},\n",
    "        {'metric': 'n_regions'},\n",
    "        {'metric': 'norm', 'p': 2, 'regions': True},\n",
    "        {'metric': 'h*'},\n",
    "        {'metric': 'h*', 'regions': True},\n",
    "        {'metric': 'entropy'},\n",
    "        {'metric': 'row_means', 'p': 2},\n",
    "        {'metric': 'col_means', 'p': 1},\n",
    "        {'metric': 'col_means', 'p': 2},\n",
    "    ]\n",
    "\n",
    "    # Prune columns that are the same\n",
    "    irm = irm[:, ~np.all(irm[1:] == irm[:-1], axis=0)]\n",
    "\n",
    "    metrics = []\n",
    "    evals = fast_evals(irm)\n",
    "    for params in metric_params:\n",
    "        if params['metric'] in ['norm', 'h*', 'entropy'] and (\n",
    "            'regions' not in list(params.keys())\n",
    "        ):\n",
    "            metrics.append(\n",
    "                score_matrix_representation(evals, is_evals=True, **params)\n",
    "            )\n",
    "        else:\n",
    "            metrics.append(\n",
    "                score_matrix_representation(irm, **params)\n",
    "            )\n",
    "    return metrics\n",
    "\n",
    "def run_network(X_train, y_train, X_test, model_params, model):\n",
    "    model = ReluNetClassifier(**model_params)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict_proba(X_train)\n",
    "    y_test_pred = model.predict_proba(X_test)\n",
    "\n",
    "    irm = model.get_internal_representation(X_train, penultimate=False)\n",
    "\n",
    "    model_metrics = get_eigenval_metrics(irm)\n",
    "    model_metrics += [\n",
    "        model.n_parameters_, len(model.hidden_layer_dims),\n",
    "        model.hidden_layer_dims[0]\n",
    "        ]\n",
    "\n",
    "    return model, y_train_pred, y_test_pred, model_metrics\n",
    "\n",
    "\n",
    "def get_y_metrics(y_true, y_pred):\n",
    "    errors = [\n",
    "        zero_one_loss(y_true, y_pred.argmax(1)),\n",
    "        np.linalg.norm(1 - y_pred[:, y_true]) / len(y_true)\n",
    "    ]\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [[\n",
    "    'hidden_layer_dims',\n",
    "    'train_01_error',\n",
    "    'train_mse',\n",
    "    'test_01_error',\n",
    "    'test_mse',   \n",
    "] + [\n",
    "    'IRM_L1', 'IRM_L2', 'n_regions', 'ACTS_L2', 'IRM_h*', 'ACTS_h*', 'entropy',\n",
    "    'rows_mean_L2', 'cols_mean_L1', 'cols_mean_L2'\n",
    "] + [\n",
    "    'n_parameters', 'depth', 'width'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK_PARAMS = {\n",
    "    'hidden_layer_dims': [\n",
    "        [4],\n",
    "        [8],\n",
    "        [16],\n",
    "        [32],\n",
    "        [40],\n",
    "        [45],\n",
    "        [47],\n",
    "        [51],\n",
    "        [55],\n",
    "        [60],\n",
    "        [100]\n",
    "    ],\n",
    "    'n_epochs': [500],# [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024],# 2048],\n",
    "    'learning_rate': [1e-5],\n",
    "    'batch_size': [128],\n",
    "    'verbose': [0],\n",
    "    'early_stop_thresh': [0],\n",
    "    \"bias\": [True],\n",
    "    # \"loss\": [torch.nn.MSELoss],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]/mnt/ssd3/ronan/miniconda3/envs/proglearn/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/mnt/ssd3/ronan/miniconda3/envs/proglearn/lib/python3.8/site-packages/numpy/core/_methods.py:180: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n",
      "100%|██████████| 11/11 [04:58<00:00, 27.14s/it]\n"
     ]
    }
   ],
   "source": [
    "keys, values = zip(*NETWORK_PARAMS.items())\n",
    "model_params_grid = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "model = None\n",
    "for model_params in tqdm(model_params_grid):\n",
    "    # Train and test model\n",
    "    model, y_train_pred, y_test_pred, model_metrics = run_network(\n",
    "        X_train, y_train, X_test, model_params, model\n",
    "    )\n",
    "\n",
    "    # Compute metrics\n",
    "    results += [\n",
    "        [model_params['hidden_layer_dims'][0]]\n",
    "        + get_y_metrics(y_train, y_train_pred)  # Train\n",
    "        + get_y_metrics(y_test, y_test_pred)  # Test\n",
    "        + model_metrics\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results[1:], columns=results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_layer_dims</th>\n",
       "      <th>train_01_error</th>\n",
       "      <th>train_mse</th>\n",
       "      <th>test_01_error</th>\n",
       "      <th>test_mse</th>\n",
       "      <th>IRM_L1</th>\n",
       "      <th>IRM_L2</th>\n",
       "      <th>n_regions</th>\n",
       "      <th>ACTS_L2</th>\n",
       "      <th>IRM_h*</th>\n",
       "      <th>ACTS_h*</th>\n",
       "      <th>entropy</th>\n",
       "      <th>rows_mean_L2</th>\n",
       "      <th>cols_mean_L1</th>\n",
       "      <th>cols_mean_L2</th>\n",
       "      <th>n_parameters</th>\n",
       "      <th>depth</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.89875</td>\n",
       "      <td>0.900858</td>\n",
       "      <td>0.9042</td>\n",
       "      <td>0.900973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3190</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.80325</td>\n",
       "      <td>0.904023</td>\n",
       "      <td>0.8012</td>\n",
       "      <td>0.903720</td>\n",
       "      <td>399.0</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>3623.037676</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2389.595605</td>\n",
       "      <td>0.099750</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>19.974984</td>\n",
       "      <td>6370</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0.70575</td>\n",
       "      <td>0.907917</td>\n",
       "      <td>0.7045</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>751.210357</td>\n",
       "      <td>14</td>\n",
       "      <td>3083.293045</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>6289.320281</td>\n",
       "      <td>0.187790</td>\n",
       "      <td>113.444444</td>\n",
       "      <td>3.602126</td>\n",
       "      <td>12730</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>0.24075</td>\n",
       "      <td>0.929540</td>\n",
       "      <td>0.2978</td>\n",
       "      <td>0.929293</td>\n",
       "      <td>5199.0</td>\n",
       "      <td>1823.520496</td>\n",
       "      <td>276</td>\n",
       "      <td>1196.600184</td>\n",
       "      <td>31</td>\n",
       "      <td>276</td>\n",
       "      <td>31664.057314</td>\n",
       "      <td>0.420941</td>\n",
       "      <td>167.709677</td>\n",
       "      <td>3.361501</td>\n",
       "      <td>25450</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>0.21675</td>\n",
       "      <td>0.935260</td>\n",
       "      <td>0.3170</td>\n",
       "      <td>0.933793</td>\n",
       "      <td>8224.0</td>\n",
       "      <td>2989.161755</td>\n",
       "      <td>507</td>\n",
       "      <td>836.669588</td>\n",
       "      <td>38</td>\n",
       "      <td>507</td>\n",
       "      <td>53364.307118</td>\n",
       "      <td>0.661154</td>\n",
       "      <td>216.421053</td>\n",
       "      <td>3.953329</td>\n",
       "      <td>31810</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>0.10825</td>\n",
       "      <td>0.941258</td>\n",
       "      <td>0.2293</td>\n",
       "      <td>0.939919</td>\n",
       "      <td>10361.0</td>\n",
       "      <td>2806.618428</td>\n",
       "      <td>1156</td>\n",
       "      <td>523.656376</td>\n",
       "      <td>44</td>\n",
       "      <td>1156</td>\n",
       "      <td>64167.834298</td>\n",
       "      <td>0.581404</td>\n",
       "      <td>235.477273</td>\n",
       "      <td>4.431235</td>\n",
       "      <td>35785</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>47</td>\n",
       "      <td>0.08400</td>\n",
       "      <td>0.941890</td>\n",
       "      <td>0.2029</td>\n",
       "      <td>0.940777</td>\n",
       "      <td>12862.0</td>\n",
       "      <td>3893.759880</td>\n",
       "      <td>1250</td>\n",
       "      <td>348.146521</td>\n",
       "      <td>45</td>\n",
       "      <td>1250</td>\n",
       "      <td>84509.718605</td>\n",
       "      <td>0.748343</td>\n",
       "      <td>285.822222</td>\n",
       "      <td>5.111401</td>\n",
       "      <td>37375</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>51</td>\n",
       "      <td>0.09775</td>\n",
       "      <td>0.941653</td>\n",
       "      <td>0.2071</td>\n",
       "      <td>0.940476</td>\n",
       "      <td>11052.0</td>\n",
       "      <td>2888.545309</td>\n",
       "      <td>1315</td>\n",
       "      <td>387.499677</td>\n",
       "      <td>51</td>\n",
       "      <td>1315</td>\n",
       "      <td>68053.643611</td>\n",
       "      <td>0.596324</td>\n",
       "      <td>216.705882</td>\n",
       "      <td>3.987582</td>\n",
       "      <td>40555</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>55</td>\n",
       "      <td>0.09925</td>\n",
       "      <td>0.941591</td>\n",
       "      <td>0.2402</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>11527.0</td>\n",
       "      <td>3131.184919</td>\n",
       "      <td>1275</td>\n",
       "      <td>351.621956</td>\n",
       "      <td>54</td>\n",
       "      <td>1275</td>\n",
       "      <td>72275.567834</td>\n",
       "      <td>0.632384</td>\n",
       "      <td>213.462963</td>\n",
       "      <td>3.858212</td>\n",
       "      <td>43735</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>60</td>\n",
       "      <td>0.05700</td>\n",
       "      <td>0.943994</td>\n",
       "      <td>0.2040</td>\n",
       "      <td>0.942454</td>\n",
       "      <td>15823.0</td>\n",
       "      <td>4292.620062</td>\n",
       "      <td>2071</td>\n",
       "      <td>236.976792</td>\n",
       "      <td>60</td>\n",
       "      <td>2071</td>\n",
       "      <td>102645.711217</td>\n",
       "      <td>0.797670</td>\n",
       "      <td>263.716667</td>\n",
       "      <td>4.651851</td>\n",
       "      <td>47710</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.948211</td>\n",
       "      <td>0.1690</td>\n",
       "      <td>0.947794</td>\n",
       "      <td>106479.0</td>\n",
       "      <td>46621.026597</td>\n",
       "      <td>4000</td>\n",
       "      <td>63.245553</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>911155.162617</td>\n",
       "      <td>3.344969</td>\n",
       "      <td>1064.790000</td>\n",
       "      <td>17.075553</td>\n",
       "      <td>79510</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hidden_layer_dims  train_01_error  train_mse  test_01_error  test_mse  \\\n",
       "0                   4         0.89875   0.900858         0.9042  0.900973   \n",
       "1                   8         0.80325   0.904023         0.8012  0.903720   \n",
       "2                  16         0.70575   0.907917         0.7045  0.908213   \n",
       "3                  32         0.24075   0.929540         0.2978  0.929293   \n",
       "4                  40         0.21675   0.935260         0.3170  0.933793   \n",
       "5                  45         0.10825   0.941258         0.2293  0.939919   \n",
       "6                  47         0.08400   0.941890         0.2029  0.940777   \n",
       "7                  51         0.09775   0.941653         0.2071  0.940476   \n",
       "8                  55         0.09925   0.941591         0.2402  0.939759   \n",
       "9                  60         0.05700   0.943994         0.2040  0.942454   \n",
       "10                100         0.00000   0.948211         0.1690  0.947794   \n",
       "\n",
       "      IRM_L1        IRM_L2  n_regions      ACTS_L2  IRM_h*  ACTS_h*  \\\n",
       "0        0.0      0.000000          1  4000.000000       0        1   \n",
       "1      399.0    399.000000          2  3623.037676       1        2   \n",
       "2     1021.0    751.210357         14  3083.293045       9       14   \n",
       "3     5199.0   1823.520496        276  1196.600184      31      276   \n",
       "4     8224.0   2989.161755        507   836.669588      38      507   \n",
       "5    10361.0   2806.618428       1156   523.656376      44     1156   \n",
       "6    12862.0   3893.759880       1250   348.146521      45     1250   \n",
       "7    11052.0   2888.545309       1315   387.499677      51     1315   \n",
       "8    11527.0   3131.184919       1275   351.621956      54     1275   \n",
       "9    15823.0   4292.620062       2071   236.976792      60     2071   \n",
       "10  106479.0  46621.026597       4000    63.245553     100        0   \n",
       "\n",
       "          entropy  rows_mean_L2  cols_mean_L1  cols_mean_L2  n_parameters  \\\n",
       "0        0.000000      0.000000           NaN           NaN          3190   \n",
       "1     2389.595605      0.099750    399.000000     19.974984          6370   \n",
       "2     6289.320281      0.187790    113.444444      3.602126         12730   \n",
       "3    31664.057314      0.420941    167.709677      3.361501         25450   \n",
       "4    53364.307118      0.661154    216.421053      3.953329         31810   \n",
       "5    64167.834298      0.581404    235.477273      4.431235         35785   \n",
       "6    84509.718605      0.748343    285.822222      5.111401         37375   \n",
       "7    68053.643611      0.596324    216.705882      3.987582         40555   \n",
       "8    72275.567834      0.632384    213.462963      3.858212         43735   \n",
       "9   102645.711217      0.797670    263.716667      4.651851         47710   \n",
       "10  911155.162617      3.344969   1064.790000     17.075553         79510   \n",
       "\n",
       "    depth  width  \n",
       "0       1      4  \n",
       "1       1      8  \n",
       "2       1     16  \n",
       "3       1     32  \n",
       "4       1     40  \n",
       "5       1     45  \n",
       "6       1     47  \n",
       "7       1     51  \n",
       "8       1     55  \n",
       "9       1     60  \n",
       "10      1    100  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = 'train_01_error'\n",
    "test_error = 'test_01_error'\n",
    "\n",
    "# Create new metrics/columns\n",
    "df['generalization_gap'] = df[test_error] - df[train_error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style('white')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, sharex=True)\n",
    "\n",
    "sns.regplot(data=df, x='n_parameters', y=test_error, lowess=False, ax=ax, label='test', fit_reg=False)\n",
    "sns.regplot(data=df, x='n_parameters', y=train_error, lowess=False, ax=ax, label='train', fit_reg=False)\n",
    "# sns.regplot(data=df, x='n_parameters', y='generalization_gap', lowess=False, ax=ax, label='gen_gap')\n",
    "\n",
    "ax.axvline(len(np.unique(y_train))*n_train, ls='--', c='black')\n",
    "plt.xscale('log')\n",
    "# plt.yscale('symlog')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generalization_gap</th>\n",
       "      <th>n_parameters</th>\n",
       "      <th>depth</th>\n",
       "      <th>width</th>\n",
       "      <th>IRM_L1</th>\n",
       "      <th>IRM_L2</th>\n",
       "      <th>n_regions</th>\n",
       "      <th>ACTS_L2</th>\n",
       "      <th>IRM_h*</th>\n",
       "      <th>ACTS_h*</th>\n",
       "      <th>entropy</th>\n",
       "      <th>rows_mean_L2</th>\n",
       "      <th>cols_mean_L1</th>\n",
       "      <th>cols_mean_L2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>generalization_gap</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>0.288889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_parameters</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>-0.927273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.244444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>width</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>-0.927273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.244444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IRM_L1</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IRM_L2</th>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>-0.927273</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.244444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_regions</th>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.890909</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>0.288889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACTS_L2</th>\n",
       "      <td>-0.818182</td>\n",
       "      <td>-0.927273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.927273</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.927273</td>\n",
       "      <td>-0.890909</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.927273</td>\n",
       "      <td>-0.527273</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.890909</td>\n",
       "      <td>-0.422222</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IRM_h*</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>-0.927273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.244444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACTS_h*</th>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>-0.527273</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>-0.022222</td>\n",
       "      <td>-0.022222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entropy</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rows_mean_L2</th>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>-0.890909</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>0.288889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cols_mean_L1</th>\n",
       "      <td>0.377778</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>-0.422222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.022222</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.911111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cols_mean_L2</th>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>-0.022222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    generalization_gap  n_parameters  depth     width  \\\n",
       "generalization_gap            1.000000      0.818182    NaN  0.818182   \n",
       "n_parameters                  0.818182      1.000000    NaN  1.000000   \n",
       "depth                              NaN           NaN    1.0       NaN   \n",
       "width                         0.818182      1.000000    NaN  1.000000   \n",
       "IRM_L1                        0.818182      0.927273    NaN  0.927273   \n",
       "IRM_L2                        0.745455      0.854545    NaN  0.854545   \n",
       "n_regions                     0.781818      0.963636    NaN  0.963636   \n",
       "ACTS_L2                      -0.818182     -0.927273    NaN -0.927273   \n",
       "IRM_h*                        0.818182      1.000000    NaN  1.000000   \n",
       "ACTS_h*                       0.418182      0.600000    NaN  0.600000   \n",
       "entropy                       0.818182      0.927273    NaN  0.927273   \n",
       "rows_mean_L2                  0.709091      0.818182    NaN  0.818182   \n",
       "cols_mean_L1                  0.377778      0.333333    NaN  0.333333   \n",
       "cols_mean_L2                  0.288889      0.244444    NaN  0.244444   \n",
       "\n",
       "                      IRM_L1    IRM_L2  n_regions   ACTS_L2    IRM_h*  \\\n",
       "generalization_gap  0.818182  0.745455   0.781818 -0.818182  0.818182   \n",
       "n_parameters        0.927273  0.854545   0.963636 -0.927273  1.000000   \n",
       "depth                    NaN       NaN        NaN       NaN       NaN   \n",
       "width               0.927273  0.854545   0.963636 -0.927273  1.000000   \n",
       "IRM_L1              1.000000  0.927273   0.890909 -1.000000  0.927273   \n",
       "IRM_L2              0.927273  1.000000   0.818182 -0.927273  0.854545   \n",
       "n_regions           0.890909  0.818182   1.000000 -0.890909  0.963636   \n",
       "ACTS_L2            -1.000000 -0.927273  -0.890909  1.000000 -0.927273   \n",
       "IRM_h*              0.927273  0.854545   0.963636 -0.927273  1.000000   \n",
       "ACTS_h*             0.527273  0.454545   0.636364 -0.527273  0.600000   \n",
       "entropy             1.000000  0.927273   0.890909 -1.000000  0.927273   \n",
       "rows_mean_L2        0.890909  0.963636   0.781818 -0.890909  0.818182   \n",
       "cols_mean_L1        0.422222  0.333333   0.377778 -0.422222  0.333333   \n",
       "cols_mean_L2        0.333333  0.244444   0.288889 -0.333333  0.244444   \n",
       "\n",
       "                     ACTS_h*   entropy  rows_mean_L2  cols_mean_L1  \\\n",
       "generalization_gap  0.418182  0.818182      0.709091      0.377778   \n",
       "n_parameters        0.600000  0.927273      0.818182      0.333333   \n",
       "depth                    NaN       NaN           NaN           NaN   \n",
       "width               0.600000  0.927273      0.818182      0.333333   \n",
       "IRM_L1              0.527273  1.000000      0.890909      0.422222   \n",
       "IRM_L2              0.454545  0.927273      0.963636      0.333333   \n",
       "n_regions           0.636364  0.890909      0.781818      0.377778   \n",
       "ACTS_L2            -0.527273 -1.000000     -0.890909     -0.422222   \n",
       "IRM_h*              0.600000  0.927273      0.818182      0.333333   \n",
       "ACTS_h*             1.000000  0.527273      0.418182     -0.022222   \n",
       "entropy             0.527273  1.000000      0.890909      0.422222   \n",
       "rows_mean_L2        0.418182  0.890909      1.000000      0.377778   \n",
       "cols_mean_L1       -0.022222  0.422222      0.377778      1.000000   \n",
       "cols_mean_L2       -0.022222  0.333333      0.288889      0.911111   \n",
       "\n",
       "                    cols_mean_L2  \n",
       "generalization_gap      0.288889  \n",
       "n_parameters            0.244444  \n",
       "depth                        NaN  \n",
       "width                   0.244444  \n",
       "IRM_L1                  0.333333  \n",
       "IRM_L2                  0.244444  \n",
       "n_regions               0.288889  \n",
       "ACTS_L2                -0.333333  \n",
       "IRM_h*                  0.244444  \n",
       "ACTS_h*                -0.022222  \n",
       "entropy                 0.333333  \n",
       "rows_mean_L2            0.288889  \n",
       "cols_mean_L1            0.911111  \n",
       "cols_mean_L2            1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr_vars = [\n",
    "    'generalization_gap',\n",
    "    'n_parameters', 'depth', 'width', # 'n_epochs',\n",
    "    'IRM_L1', 'IRM_L2', 'n_regions', 'ACTS_L2',\n",
    "    'IRM_h*', 'ACTS_h*', 'entropy',\n",
    "    'rows_mean_L2',\n",
    "    'cols_mean_L1', 'cols_mean_L2',\n",
    "]\n",
    "\n",
    "corr_df = df[corr_vars].corr('kendall')\n",
    "display(corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (proglearn)",
   "language": "python",
   "name": "proglearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
