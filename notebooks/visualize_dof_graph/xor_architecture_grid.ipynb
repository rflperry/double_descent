{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from collections import defaultdict\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../functions/')\n",
    "from dataset import generate_gaussian_parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polytope functions, tensorflow version\n",
    "def get_activations(model, X, penultimate=False):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    From a ReLU neural network and a training dataset, computes the number of polytopes\n",
    "    occupied by the training samples relative to the sample size. Each polytope\n",
    "    corresponds to a fixed assignment of all ReLU's as either on/off and within each\n",
    "    polytope the network is a linear function. One may think of this fraction\n",
    "    as the relative number of piecewise linear functions a network has to learn on the\n",
    "    data in order to perform well.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : pytorch sequential network with ReLUs\n",
    "    X : training data loader\n",
    "    penultimate : boolean, default False\n",
    "        If True, only returns polytopes using the last layer of ReLUs. Set to True\n",
    "        if all the prior layers are viewed as a representation learner.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    fraction : float\n",
    "        Fraction of training samples in unique polytopes.\n",
    "    polytope_assignments : list, length=dataloader_sample_size\n",
    "        Labels encoding which samples occured in which polytopes.\n",
    "    \"\"\"\n",
    "    polytope_memberships = []\n",
    "    n_samples = 0\n",
    "    \n",
    "    n_samples += X.shape[0]\n",
    "    polytope_memberships = []\n",
    "    for layer in model.layers[:-1]: # Assumes sequential, may have to adjust based on model\n",
    "        X = layer(X)\n",
    "        binary_preactivation = (X.numpy() > 0.0).astype('int')\n",
    "        polytope_memberships.append(binary_preactivation)\n",
    "    \n",
    "    if penultimate:\n",
    "        polytope_memberships = polytope_memberships[-1]\n",
    "    else:\n",
    "        polytope_memberships = np.hstack(polytope_memberships)\n",
    "        \n",
    "    return polytope_memberships\n",
    "    \n",
    "#     polytopes, assignments, counts = np.unique(polytope_memberships, axis=0, return_inverse=True, return_counts=True)\n",
    "    \n",
    "#     # kernel_mat = 1 - squareform(pdist(polytope_memberships, p=1)) / polytope_memberships.shape[1]\n",
    "#     # kernel_mat = polytope_memberships @ polytope_memberships.T / polytope_memberships.shape[1]\n",
    "#     # mat_evals = np.linalg.svd(kernel_mat, compute_uv=False, hermitian=True)\n",
    "#     mat_evals = np.linalg.svd(polytope_memberships / np.sqrt(polytope_memberships.shape[1]), compute_uv=False)**2\n",
    "    \n",
    "#     return len(polytopes) / n_samples, assignments, counts, entropy(mat_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train = make_moons(1000)\n",
    "X_train, y_train = generate_gaussian_parity(n=1000, angle_params=0, acorn=1234)\n",
    "X_test, y_test = generate_gaussian_parity(n=10000, angle_params=0, acorn=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model depth=1, width=4, train_acc=0.688, test_acc=0.6734\n",
      "Model depth=1, width=8, train_acc=0.729, test_acc=0.7241\n",
      "Model depth=1, width=16, train_acc=0.738, test_acc=0.7271\n",
      "Model depth=1, width=32, train_acc=0.742, test_acc=0.7272\n",
      "Model depth=2, width=4, train_acc=0.733, test_acc=0.7284\n",
      "Model depth=2, width=8, train_acc=0.743, test_acc=0.7298\n",
      "Model depth=2, width=16, train_acc=0.747, test_acc=0.7284\n",
      "Model depth=2, width=32, train_acc=0.745, test_acc=0.7281\n",
      "Model depth=3, width=4, train_acc=0.732, test_acc=0.7142\n",
      "Model depth=3, width=8, train_acc=0.742, test_acc=0.7322\n",
      "Model depth=3, width=16, train_acc=0.753, test_acc=0.7301\n",
      "Model depth=3, width=32, train_acc=0.752, test_acc=0.7251\n",
      "Model depth=4, width=4, train_acc=0.647, test_acc=0.6175\n",
      "Model depth=4, width=8, train_acc=0.751, test_acc=0.73\n",
      "Model depth=4, width=16, train_acc=0.752, test_acc=0.7288\n",
      "Model depth=4, width=32, train_acc=0.756, test_acc=0.7253\n"
     ]
    }
   ],
   "source": [
    "widths =  [4, 8, 16, 32]\n",
    "depths = [1, 2, 3, 4]\n",
    "\n",
    "for depth in depths:\n",
    "    for width in widths:\n",
    "        layers = [tf.keras.layers.Dense(width, activation='relu') for _ in range(depth)]\n",
    "        model = tf.keras.models.Sequential(layers + [\n",
    "            tf.keras.layers.Dense(2)\n",
    "        ])\n",
    "\n",
    "        n_epochs = 100\n",
    "        optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_fn,\n",
    "            metrics=[metric],\n",
    "        )\n",
    "\n",
    "        history = model.fit(X_train, y_train, epochs=n_epochs, verbose=0)\n",
    "\n",
    "        test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0) # Loss, accuracy\n",
    "        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0) # Loss, accuracy\n",
    "\n",
    "        print(f\"Model depth={depth}, width={width}, train_acc={train_acc}, test_acc={test_acc}\")\n",
    "        \n",
    "        activations = get_activations(model, X_train)\n",
    "        \n",
    "        results_dict = {\n",
    "            \"activations\": activations,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "        }\n",
    "\n",
    "        with open(f'./activation_matrices/xor_results_depth={depth}_width={width}.pkl', 'wb') as f:\n",
    "            pickle.dump(results_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
