{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba510c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys; sys.path.append('../../PGDL/sample_code_submission/')\n",
    "from internal_rep.matrix_funcs import \\\n",
    "    get_KF_Schatten_norms, \\\n",
    "    compute_complexity, \\\n",
    "    get_df_tau, \\\n",
    "    evalues_from_regions, \\\n",
    "    get_local_rad_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "386785bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(x, y, n_samples, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    else:\n",
    "        np.random.seed(int(time.time()))\n",
    "\n",
    "    indices = np.random.choice(range(x.shape[0]), size=n_samples, replace=False)\n",
    "    x = x[indices]\n",
    "    y = y[indices]\n",
    "    return x, y\n",
    "\n",
    "def prepare_mnist_dataset(\n",
    "    batch_size=64,\n",
    "    train_sample_size=None,\n",
    "    test_sample_size=None,\n",
    "    seed=None,\n",
    "    shuffle=True,\n",
    "    shuffle_label_frac=None\n",
    "):\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()#path='/mnt/ssd3/ronan/tensorflow_datasets/')\n",
    "    y_train = y_train.astype('int32')\n",
    "    y_test = y_test.astype('int32')\n",
    "\n",
    "    if train_sample_size is not None:\n",
    "        x_train, y_train = sample_dataset(x_train, y_train, int(train_sample_size), seed)\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.arange(train_sample_size)\n",
    "            np.random.seed(seed)\n",
    "            np.random.shuffle(shuffle_indices)\n",
    "            x_train = x_train[shuffle_indices]\n",
    "            y_train = y_train[shuffle_indices]\n",
    "        if shuffle_label_frac is not None:\n",
    "            n_shuffle = int(shuffle_label_frac * train_sample_size)\n",
    "            y_shuffle = y_train[:n_shuffle]\n",
    "            np.random.seed(seed)\n",
    "            np.random.shuffle(y_shuffle)\n",
    "            y_train[:n_shuffle] = y_shuffle\n",
    "\n",
    "    if test_sample_size is not None:\n",
    "        x_test, y_test = sample_dataset(x_test, y_test, int(test_sample_size), seed)\n",
    "\n",
    "    x_train, x_test = tf.cast(x_train, tf.float32) / 255.0, tf.cast(x_test, tf.float32) / 255.0\n",
    "    print(f\"x_train.shape={x_train.shape} y_train={y_train.shape} \"\n",
    "                 f\"x_test.shape={x_test.shape} y_test={y_test.shape}\")\n",
    "\n",
    "    shuffle_buffer = 1000\n",
    "    prefetch_buffer = 1000\n",
    "    dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    dataset_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    # dataset = dataset.shuffle(shuffle_buffer)\n",
    "    dataset_train = dataset_train.prefetch(prefetch_buffer).batch(batch_size)\n",
    "    dataset_test = dataset_test.prefetch(prefetch_buffer).batch(batch_size)\n",
    "    \n",
    "    return dataset_train, dataset_test # , (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bb01469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, n_epochs, optimizer, loss_fn, metric, metric_dict={}, save_path=None, epoch_save=5):\n",
    "    @tf.function\n",
    "    def _train_step(x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x, training=True)\n",
    "            loss_value = loss_fn(y, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Update training metric\n",
    "        metric.update_state(y, logits)\n",
    "        return loss_value\n",
    "\n",
    "    # @tf.function\n",
    "    def _test_step(x, y):\n",
    "        logits = model(x, training=False)\n",
    "        # Update val metrics\n",
    "        metric.update_state(y, logits)\n",
    "        return logits, y\n",
    "\n",
    "    model_results = defaultdict(list)\n",
    "    model_results['epochs'] = n_epochs\n",
    "    \n",
    "    train_eval_mat = []\n",
    "    test_eval_mat = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        losses = []\n",
    "        for step, (x_batch, y_batch) in enumerate(ds_train):\n",
    "            loss_value = _train_step(x_batch, y_batch)\n",
    "            losses.append(loss_value)\n",
    "        model_results['train_loss'].append(np.mean(losses))\n",
    "            \n",
    "        # Display metrics at the end of each epoch.\n",
    "        train_acc = metric.result()\n",
    "        metric.reset_states()\n",
    "        \n",
    "        model_results['train_accuracy'].append(train_acc)\n",
    "        \n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        logit_list = []\n",
    "        y_list = []\n",
    "        for x_batch, y_batch in ds_test:\n",
    "            logits, y = _test_step(x_batch, y_batch)\n",
    "            logit_list.append(logits)\n",
    "            y_list.append(y)\n",
    "        y_list = tf.concat(y_list, axis=0)\n",
    "        logit_list = tf.concat(logit_list, axis=0)\n",
    "        model_results['test_ece'].append(tfp.stats.expected_calibration_error(10, logit_list, y_list))\n",
    "        \n",
    "            \n",
    "        test_acc = metric.result()\n",
    "        metric.reset_states()\n",
    "        model_results['test_accuracy'].append(test_acc)\n",
    "        \n",
    "        # Get the TRAIN complexity at the end of each epoch\n",
    "        for label, ds in zip(('train', 'test'), (ds_train, ds_test)):\n",
    "            internal_rep = []\n",
    "            for x_batch, y_batch in ds:\n",
    "                for layer in model.layers[:-1]:\n",
    "                    x_batch = layer(x_batch)\n",
    "                internal_rep.append((x_batch.numpy() > 0).astype('bool'))\n",
    "            internal_rep = np.vstack(internal_rep)\n",
    "            evalues = evalues_from_regions(internal_rep)\n",
    "            if epoch % epoch_save == 0:\n",
    "                if label == 'train':\n",
    "                    train_eval_mat.append(evalues)\n",
    "                elif label == 'test':\n",
    "                    test_eval_mat.append(evalues)\n",
    "                else:\n",
    "                    raise ValueError()\n",
    "                \n",
    "            h_star, h_argmin = get_local_rad_bound(evalues, from_evalues=True)\n",
    "            model_results[f'h*_{label}'].append(h_star)\n",
    "            model_results[f'h_argmin_{label}'].append(h_argmin)\n",
    "            model_results[f'n_activated_regions_{label}'].append(sum(evalues > 0))\n",
    "        \n",
    "        # Get the TEST complexity at the end of each epoch\n",
    "\n",
    "        \n",
    "        if epoch % epoch_save == 0:\n",
    "            print(f\"Epoch {epoch}: Training acc={train_acc:.3f}, Validation acc={test_acc:.3f}\")\n",
    "        \n",
    "    if save_path is not None:\n",
    "        np.save(save_path + '_train_evalues.npy', train_eval_mat)\n",
    "        np.save(save_path + '_test_evalues.npy', test_eval_mat)\n",
    "        with open(save_path + '_results_dict.pkl', 'wb') as f:\n",
    "            pickle.dump(model_results, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    return model, model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4899a3",
   "metadata": {},
   "source": [
    "## Run and save models, varying width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b450e177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape=(1000, 28, 28) y_train=(1000,) x_test.shape=(10000, 28, 28) y_test=(10000,)\n",
      "Epoch 0: Training acc=0.150, Validation acc=0.203\n",
      "Epoch 5: Training acc=0.427, Validation acc=0.416\n",
      "x_train.shape=(1000, 28, 28) y_train=(1000,) x_test.shape=(10000, 28, 28) y_test=(10000,)\n",
      "Epoch 0: Training acc=0.170, Validation acc=0.283\n",
      "Epoch 5: Training acc=0.560, Validation acc=0.627\n",
      "x_train.shape=(1000, 28, 28) y_train=(1000,) x_test.shape=(10000, 28, 28) y_test=(10000,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ad12ee9a6683>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         seed=0)\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     model, model_results = fit_model(\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         save_path=f'./width_results/relu_net_units={n_units}')\n",
      "\u001b[0;32m<ipython-input-5-a3f071034173>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model, n_epochs, optimizer, loss_fn, metric, metric_dict, save_path, epoch_save)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0my_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_test_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mlogit_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0my_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a3f071034173>\u001b[0m in \u001b[0;36m_test_step\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# @tf.function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_test_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Update val metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/proglearn/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/proglearn/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/proglearn/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \"\"\"\n\u001b[0;32m--> 424\u001b[0;31m     return self._run_internal_graph(\n\u001b[0m\u001b[1;32m    425\u001b[0m         inputs, training=training, mask=mask)\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/proglearn/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/proglearn/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0mcall_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autographed_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/proglearn/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   6630\u001b[0m       \u001b[0;31m# fully specified scope name, for compatibility with Graph.name_scope.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6631\u001b[0m       \u001b[0;31m# This also prevents auto-incrementing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6632\u001b[0;31m       \u001b[0mold_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6633\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6634\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/proglearn/lib/python3.8/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mscope_name\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[0;34m\"\"\"Returns scope name for the current thread.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mscope_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_train_sample = 1000\n",
    "n_units_list = [4, 8, 16, 32, 48, 64, 80, 100]\n",
    "n_epochs = 200\n",
    "\n",
    "for n_units in n_units_list:\n",
    "    total_params = (28 * 28 + 1) * n_units + (n_units + 1) * 10\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(),#input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(n_units, activation='relu'),\n",
    "        tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "    \n",
    "    # model = tf.keras.models.clone_model(model_base)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    \n",
    "    ds_train, ds_test = prepare_mnist_dataset(\n",
    "        train_sample_size=n_train_sample, test_sample_size=10000,\n",
    "        seed=0)\n",
    "    \n",
    "    model, model_results = fit_model(\n",
    "        model, n_epochs, optimizer, loss_fn, metric,\n",
    "        save_path=f'./width_results/relu_net_units={n_units}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a126b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (proglearn)",
   "language": "python",
   "name": "proglearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
